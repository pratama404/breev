{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Index (AQI) Data Exploration and Preprocessing\n",
    "## Dataset: Jakarta Air Quality 2010-2025\n",
    "### Source: https://www.kaggle.com/datasets/senadu34/air-quality-index-in-jakarta-2010-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main dataset\n",
    "df = pd.read_csv('../data/ispu_dki_all.csv')\n",
    "\n",
    "# Create a backup copy\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n=== LAST 5 ROWS ===\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Wrangling\n",
    "### 4.1 Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gathering summary\n",
    "print(\"=== DATA GATHERING SUMMARY ===\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df['tanggal'].min()} to {df['tanggal'].max()}\")\n",
    "print(f\"Unique stations: {df['stasiun'].nunique()}\")\n",
    "print(f\"Station names: {df['stasiun'].unique()}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Assessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values assessment\n",
    "print(\"=== MISSING VALUES ASSESSMENT ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "print(missing_df)\n",
    "\n",
    "# Duplicate assessment\n",
    "print(f\"\\n=== DUPLICATE ASSESSMENT ===\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Total duplicates: {duplicates}\")\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "print(\"=== DATETIME CONVERSION ===\")\n",
    "df['tanggal'] = pd.to_datetime(df['tanggal'])\n",
    "print(f\"Date column converted to datetime\")\n",
    "print(f\"Date range: {df['tanggal'].min()} to {df['tanggal'].max()}\")\n",
    "\n",
    "# Extract date features\n",
    "df['year'] = df['tanggal'].dt.year\n",
    "df['month'] = df['tanggal'].dt.month\n",
    "df['day'] = df['tanggal'].dt.day\n",
    "df['dayofweek'] = df['tanggal'].dt.dayofweek\n",
    "df['quarter'] = df['tanggal'].dt.quarter\n",
    "\n",
    "print(\"Date features extracted: year, month, day, dayofweek, quarter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Values and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"=== HANDLING MISSING VALUES ===\")\n",
    "numeric_columns = ['pm25', 'pm10', 'so2', 'co', 'o3', 'no2', 'max']\n",
    "\n",
    "# Fill missing values with median for numeric columns\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        missing_before = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        print(f\"{col}: {missing_before} missing values filled with median\")\n",
    "\n",
    "# Handle categorical missing values\n",
    "if 'critical' in df.columns:\n",
    "    df['critical'] = df['critical'].fillna('Unknown')\n",
    "    print(f\"Critical column: missing values filled with 'Unknown'\")\n",
    "\n",
    "if 'category' in df.columns:\n",
    "    df['category'] = df['category'].fillna('Unknown')\n",
    "    print(f\"Category column: missing values filled with 'Unknown'\")\n",
    "\n",
    "print(f\"\\nMissing values after cleaning: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "### 6.1 Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of pollutants\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "pollutants = ['pm25', 'pm10', 'so2', 'co', 'o3', 'no2']\n",
    "\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    if pollutant in df.columns:\n",
    "        axes[i].hist(df[pollutant].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'Distribution of {pollutant.upper()}')\n",
    "        axes[i].set_xlabel(pollutant.upper())\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Air Quality Parameters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Strong correlations\n",
    "print(\"=== STRONG CORRELATIONS (>0.5) ===\")\n",
    "strong_corr = correlation_matrix.abs() > 0.5\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if strong_corr.iloc[i, j]:\n",
    "            print(f\"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis\n",
    "print(\"=== STATISTICAL ANALYSIS ===\")\n",
    "stats_df = df[pollutants].describe()\n",
    "display(stats_df)\n",
    "\n",
    "# Skewness and Kurtosis\n",
    "print(\"\\n=== SKEWNESS AND KURTOSIS ===\")\n",
    "skew_kurt = pd.DataFrame({\n",
    "    'Skewness': df[pollutants].skew(),\n",
    "    'Kurtosis': df[pollutants].kurtosis()\n",
    "})\n",
    "display(skew_kurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    if pollutant in df.columns:\n",
    "        axes[i].boxplot(df[pollutant].dropna())\n",
    "        axes[i].set_title(f'Box Plot of {pollutant.upper()}')\n",
    "        axes[i].set_ylabel(pollutant.upper())\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "df_monthly = df.groupby(['year', 'month'])[pollutants].mean().reset_index()\n",
    "df_monthly['date'] = pd.to_datetime(df_monthly[['year', 'month']].assign(day=1))\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    axes[i].plot(df_monthly['date'], df_monthly[pollutant], marker='o', linewidth=2)\n",
    "    axes[i].set_title(f'Monthly Average {pollutant.upper()} Over Time')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel(f'{pollutant.upper()}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot for multivariate analysis\n",
    "sample_df = df[pollutants].sample(n=min(1000, len(df)))  # Sample for performance\n",
    "sns.pairplot(sample_df, diag_kind='hist', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Pairwise Relationships Between Pollutants', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Data Preprocessing\n",
    "### 7.1 Remove Missing Values and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(\"=== REMOVING DUPLICATES ===\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after_dup = len(df)\n",
    "print(f\"Removed {before_dup - after_dup} duplicate rows\")\n",
    "\n",
    "# Final missing value check\n",
    "print(\"\\n=== FINAL MISSING VALUE CHECK ===\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(\"=== OUTLIER DETECTION ===\")\n",
    "outlier_summary = {}\n",
    "\n",
    "for pollutant in pollutants:\n",
    "    if pollutant in df.columns:\n",
    "        outliers, lower, upper = detect_outliers_iqr(df, pollutant)\n",
    "        outlier_summary[pollutant] = {\n",
    "            'count': len(outliers),\n",
    "            'percentage': (len(outliers) / len(df)) * 100,\n",
    "            'lower_bound': lower,\n",
    "            'upper_bound': upper\n",
    "        }\n",
    "        print(f\"{pollutant}: {len(outliers)} outliers ({(len(outliers)/len(df)*100):.2f}%)\")\n",
    "\n",
    "# Cap outliers instead of removing them\n",
    "df_clean = df.copy()\n",
    "for pollutant in pollutants:\n",
    "    if pollutant in df_clean.columns:\n",
    "        lower = outlier_summary[pollutant]['lower_bound']\n",
    "        upper = outlier_summary[pollutant]['upper_bound']\n",
    "        df_clean[pollutant] = df_clean[pollutant].clip(lower=lower, upper=upper)\n",
    "\n",
    "print(\"\\nOutliers capped to IQR bounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = pollutants + ['max']\n",
    "scaled_features = [col for col in scaled_features if col in df_clean.columns]\n",
    "\n",
    "df_scaled = df_clean.copy()\n",
    "df_scaled[scaled_features] = scaler.fit_transform(df_clean[scaled_features])\n",
    "\n",
    "print(\"=== FEATURE SCALING COMPLETED ===\")\n",
    "print(f\"Scaled features: {scaled_features}\")\n",
    "print(\"\\nScaled data statistics:\")\n",
    "display(df_scaled[scaled_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le_critical = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "\n",
    "if 'critical' in df_clean.columns:\n",
    "    df_clean['critical_encoded'] = le_critical.fit_transform(df_clean['critical'])\n",
    "    print(f\"Critical pollutant encoded: {dict(zip(le_critical.classes_, le_critical.transform(le_critical.classes_)))}\")\n",
    "\n",
    "if 'category' in df_clean.columns:\n",
    "    df_clean['category_encoded'] = le_category.fit_transform(df_clean['category'])\n",
    "    print(f\"Category encoded: {dict(zip(le_category.classes_, le_category.transform(le_category.classes_)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Feature Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AQI categories based on max values\n",
    "def categorize_aqi(value):\n",
    "    if value <= 50:\n",
    "        return 'Good'\n",
    "    elif value <= 100:\n",
    "        return 'Moderate'\n",
    "    elif value <= 150:\n",
    "        return 'Unhealthy for Sensitive Groups'\n",
    "    elif value <= 200:\n",
    "        return 'Unhealthy'\n",
    "    elif value <= 300:\n",
    "        return 'Very Unhealthy'\n",
    "    else:\n",
    "        return 'Hazardous'\n",
    "\n",
    "if 'max' in df_clean.columns:\n",
    "    df_clean['aqi_category'] = df_clean['max'].apply(categorize_aqi)\n",
    "    print(\"=== AQI CATEGORIES CREATED ===\")\n",
    "    print(df_clean['aqi_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned datasets\n",
    "print(\"=== SAVING CLEANED DATA ===\")\n",
    "\n",
    "# Save original cleaned data\n",
    "df_clean.to_csv('../data/aqi_cleaned.csv', index=False)\n",
    "print(\"Cleaned data saved to: ../data/aqi_cleaned.csv\")\n",
    "\n",
    "# Save scaled data\n",
    "df_scaled.to_csv('../data/aqi_scaled.csv', index=False)\n",
    "print(\"Scaled data saved to: ../data/aqi_scaled.csv\")\n",
    "\n",
    "# Save preprocessing objects\n",
    "import joblib\n",
    "joblib.dump(scaler, '../artifacts/scaler.pkl')\n",
    "if 'critical' in df_clean.columns:\n",
    "    joblib.dump(le_critical, '../artifacts/label_encoder_critical.pkl')\n",
    "if 'category' in df_clean.columns:\n",
    "    joblib.dump(le_category, '../artifacts/label_encoder_category.pkl')\n",
    "\n",
    "print(\"Preprocessing objects saved to: ../artifacts/\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n=== FINAL DATASET SUMMARY ===\")\n",
    "print(f\"Original shape: {df_original.shape}\")\n",
    "print(f\"Cleaned shape: {df_clean.shape}\")\n",
    "print(f\"Features: {list(df_clean.columns)}\")\n",
    "print(f\"Data types: {df_clean.dtypes.value_counts()}\")\n",
    "print(\"\\nData preprocessing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
